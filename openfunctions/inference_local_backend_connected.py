import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from openfunctions_utils import strip_function_calls, parse_function_call
from global_variables import get_server_input, put_llm_response, server_communication_init
from server import run_server


def get_prompt(system, user_query: str, functions: list = []) -> str:
    """
    Generates a conversation prompt based on the user's query and a list of functions.

    Parameters:
    - user_query (str): The user's query.
    - functions (list): A list of functions to include in the prompt.

    Returns:
    - str: The formatted conversation prompt.
    """
    # system = "You are an AI programming assistant, utilizing the Gorilla LLM model, developed by Gorilla LLM, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer."
    if len(functions) == 0:
        return f"{system}\n### Instruction: <<question>> {user_query}\n### Response: "
    functions_string = json.dumps(functions)
    return f"{system}\n### Instruction: <<function>>{functions_string}\n<<question>>{user_query}\n### Response: "


def format_response(response: str):
    """
    Formats the response from the OpenFunctions model.

    Parameters:
    - response (str): The response generated by the LLM.

    Returns:
    - str: The formatted response.
    - dict: The function call(s) extracted from the response.

    """
    function_call_dicts = None
    try:
        response = strip_function_calls(response)
        # Parallel function calls returned as a str, list[dict]
        if len(response) > 1: 
            function_call_dicts = []
            for function_call in response:
                function_call_dicts.append(parse_function_call(function_call))
            response = ", ".join(response)
        # Single function call returned as a str, dict
        else:
            function_call_dicts = parse_function_call(response[0])
            response = response[0]
    except Exception as e:
        # Just faithfully return the generated response str to the user
        pass
    return response, function_call_dicts

def initialize_pipe():
    # Device setup
    device : str = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    print("is cuda available? ", torch.cuda.is_available())
    # Model and tokenizer setup
    model_id : str = "gorilla-llm/gorilla-openfunctions-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True)

    # Move model to device
    model.to(device)

    # Pipeline setup
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=128,
        batch_size=16,
        torch_dtype=torch_dtype,
        device=device
    )
    return pipe

def create_prompt_and_function_descriptions(user_input_prompt, current_menu):
    full_prompt = user_input_prompt
    egress_functions_backend = [
    {
  	"name": "on_egress_menu_do_next_task",
  	"description": "perform the next egress task",
  	"parameters": {
    	"required": []
  	    }
	},
    {
  	"name": "on_egress_menu_do_previous_task",
  	"description": "perform the previous egress task",
  	"parameters": {
    	"required": []
  	    }
	},
    {
  	"name": "on_egress_menu_do_current_task",
  	"description": "perform the current egress task again",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_1a",
  	"description": "perform on egress subtask 1a, this is also the start of egress task",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_1b",
  	"description": "perform on egress subtask 1b",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_1c",
  	"description": "perform on egress subtask 1c",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_2",
  	"description": "perform on egress subtask 2",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_3a",
  	"description": "perform on egress subtask 3a",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_3b",
  	"description": "perform on egress subtask 3b",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_3c",
  	"description": "perform on egress subtask 3c",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4a1",
  	"description": "perform on egress subtask 4a1",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4a2",
  	"description": "perform on egress subtask 4a2",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4a3",
  	"description": "perform on egress subtask 4a3",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4a4",
  	"description": "perform on egress subtask 4a4",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4b1",
  	"description": "perform on egress subtask 4b1",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4b2",
  	"description": "perform on egress subtask 4b2",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4b3",
  	"description": "perform on egress subtask 4b3",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4b4",
  	"description": "perform on egress subtask 4b4",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_4c",
  	"description": "perform on egress subtask 4c",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5a",
  	"description": "perform on egress subtask 5a",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5b1",
  	"description": "perform on egress subtask 5b1",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5b2",
  	"description": "perform on egress subtask 5b2",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5b3",
  	"description": "perform on egress subtask 5b3",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5c1",
  	"description": "perform on egress subtask 5c1",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5c2",
  	"description": "perform on egress subtask 5c2",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5c3",
  	"description": "perform on egress subtask 5c3",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_5d",
  	"description": "perform on egress subtask 5d",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_6a",
  	"description": "perform on egress subtask 6a",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_6b",
  	"description": "perform on egress subtask 6b",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_7a",
  	"description": "perform on egress subtask 7a",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_7b",
  	"description": "perform on egress subtask 7b",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_7c",
  	"description": "perform on egress subtask 7c",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_7d",
  	"description": "perform on egress subtask 7d",
  	"parameters": {
    	"required": []
  	    }   
	},
	{
  	"name": "on_egress_menu_do_subtask_7e",
  	"description": "perform on egress subtask 7e",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_7f",
  	"description": "perform on egress subtask 7f",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_8a",
  	"description": "perform on egress subtask 8a",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_8b",
  	"description": "perform on egress subtask 8b",
  	"parameters": {
    	"required": []
  	    }
	},
	{
  	"name": "on_egress_menu_do_subtask_9",
  	"description": "perform on egress subtask 9",
  	"parameters": {
    	"required": []
  	    }
	}
    ]
    

    # ######################################## INGRESS ###################################################################
    ingress_functions_backend = [
		{
		"name": "on_ingress_menu_do_next_task",
		"description": "perform the next ingress task",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_previous_task",
		"description": "perform the previous ingress task",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_current_task",
		"description": "perform the current ingress task again",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_1a",
		"description": "perform on ingress subtask 1a",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_1b",
		"description": "perform on ingress subtask 1b",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_1c",
		"description": "perform on ingress subtask 1c",
		"parameters": {
			"required": []
			}   
		},
		{
		"name": "on_ingress_menu_do_subtask_2a",
		"description": "perform on ingress subtask 2a",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_2b",
		"description": "perform on ingress subtask 2b",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_2c",
		"description": "perform on ingress subtask 2c",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_3a",
		"description": "perform on ingress subtask 3a",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_3b",
		"description": "perform on ingress subtask 3b",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_3c",
		"description": "perform on ingress subtask 3c",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_3d",
		"description": "perform on ingress subtask 3d",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_4a",
		"description": "perform on ingress subtask 4a",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_4b",
		"description": "perform on ingress subtask 4b",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_ingress_menu_do_subtask_5",
		"description": "perform on ingress subtask 5",
		"parameters": {
			"required": []
			}
		}
    ]
    

    # ######################################## NAVIGATION ###################################################################
    navigation_functions_backend = [
		{
		"name": "on_navigation_open_map",
		"description": "open a 2d map showing my current location, and the location of all stations",
		"parameters": {
			"required": []
			}
		},
		{
		"name": "on_navigation_remove_pin",
		"description": "Remove pin with the given pin number",
		"parameters": { 
			"pin_number": 
				{"type": "integer", 
				"description": "number of the pin to remove"
				},
			"required": ["pin_number"],
			},
		},
        {
		"name": "on_navigation_pin_my_location",
		"description": "Pin with the current location on map with the given pin number",
		"parameters": { 
			"pin_number": 
				{"type": "integer", 
				"description": "number of the pin to add"
				},
			"required": [],
			},
		},
		{
		"name": "on_navigation_return_to_airlock",
		"description": "Assist me to head back to the airlock by showing the path to follow",
		"parameters": {
			"required": []
			}
		}
    ]
    
    if current_menu == "egress":
        functions_backend = egress_functions_backend
    elif current_menu == "ingress":
        functions_backend = ingress_functions_backend
    else:
        functions_backend = navigation_functions_backend
    print("functions_backend", functions_backend, "full p", full_prompt)
    return full_prompt, functions_backend

pipe = initialize_pipe()
server_communication_init()
run_server()
# try to move this on server
current_menu = "navigation"

# def on_egress_menu_do_next_task():
#     curr_task = "1a"
    

while True: 
	input_msg = get_server_input()
	while not input_msg:
		input_msg = get_server_input()
	
	print("input_msg", input_msg, "task ", current_menu)
	full_prompt, functions_backend = create_prompt_and_function_descriptions(input_msg, current_menu)
	system_prompt = "You are an AI programming assistant, you are on menu {}".format(current_menu)
	prompt_1 = get_prompt(system_prompt, input_msg, functions=functions_backend)
	output_1 = pipe(prompt_1)
	fn_call_string, function_call_dict = format_response(output_1[0]['generated_text'])
	print("--------------------")
	print(f"User input is {input_msg}")
	print(f"Full prompt is input is {full_prompt}")
	print(f"Function call strings 1(s): {fn_call_string}")
	print(f"OpenAI compatible `function_call`: {function_call_dict}")
	print("--------------------")
	put_llm_response(function_call_dict)